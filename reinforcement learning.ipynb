{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MDP:\n",
    "    def __init__(self, nb_questions=4, win_proba=[9/10, 3/4, 1/2, 1/10], reward=[100,1000,10000,50000]):\n",
    "        self.nb_questions = nb_questions\n",
    "        self.play_probs = win_proba\n",
    "        self.rewards = reward\n",
    "        self.q_table = np.zeros((nb_questions, 2))\n",
    "        self.reward_table = np.zeros((nb_questions, 2)) # IMMEDIATE reward table\n",
    "\n",
    "\n",
    "    def call_action(self, question):\n",
    "        return self.rewards[question]*np.random.binomial(1, self.play_probs[question], 1)\n",
    "\n",
    "    def bellman_op(self, gamma=0.99):\n",
    "        cum_sum = 0\n",
    "        for i in range(self.nb_questions):  # Evaluate cumulative rewards per action and state\n",
    "            for j in range(2):\n",
    "                self.reward_table[i,j] = (1-j)*cum_sum + j*(cum_sum+self.rewards[i])\n",
    "                cum_sum += j*self.rewards[i]\n",
    "        self.reward_table[:,1] = 0\n",
    "\n",
    "        for i in range(self.nb_questions-1, -1, -1):    # Compute from last question\n",
    "            for j in range(2):\n",
    "                if i==self.nb_questions-1:\n",
    "                    self.q_table[i,j] = ((1-j)*(self.reward_table[i,j]) # Quit\n",
    "                                         + j*(self.play_probs[i]*cum_sum))   # Play\n",
    "                else:\n",
    "                    self.q_table[i,j] = (\n",
    "                        (1-j)*(self.reward_table[i,j])  # Quit\n",
    "                        + j*(self.play_probs[i]*(self.reward_table[i,j] + gamma*np.max(self.q_table[i+1]))) # Play\n",
    "                    )\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        self.question = 0   # The rank of the question\n",
    "        self.won = False\n",
    "        self.lost = False\n",
    "        self.summed_reward = 0\n",
    "        self.env = environment\n",
    "        print(\"Welcome to the game! You are at question 1. Do you play?\")\n",
    "\n",
    "    def play(self):\n",
    "        if not self.lost:\n",
    "            immediate_reward = self.env.call_action(self.question)\n",
    "            self.summed_reward += immediate_reward\n",
    "            if immediate_reward == 0:\n",
    "                self.lost = True\n",
    "                print(\"You lost :( \\n   You exit the game with a gain of {}\".format(self.summed_reward))\n",
    "            elif self.question==self.env.nb_questions:\n",
    "                print(\"Congratulations, you won!! :) \\n   Your total gains amount to {}\".format(self.summed_reward))\n",
    "            else:\n",
    "                self.question += 1\n",
    "                print(\"You passed! You are now at question {}. \\n   Your total gains are {}\".format(self.question+1, self.summed_reward))\n",
    "        else:\n",
    "            print(\"Bro the game is over... Take your {} euros and go home now.\".format(self.summed_reward))\n",
    "\n",
    "    def quit(self):\n",
    "        if not self.won:\n",
    "            self.lost = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.          3634.98262875]\n",
      " [  100.          4079.66625   ]\n",
      " [ 1100.          5494.5       ]\n",
      " [11100.          6110.        ]]\n",
      "[[    0.     0.]\n",
      " [  100.     0.]\n",
      " [ 1100.     0.]\n",
      " [11100.     0.]]\n"
     ]
    }
   ],
   "source": [
    "environment = MDP()\n",
    "environment.bellman_op()\n",
    "print(environment.q_table)\n",
    "print(environment.reward_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Theoretical Frame:\n",
    "1) Start at $V(s) = 0$ for every state.\n",
    "2) Run through the simulation choosing an action as $\\text{arg max}_{s'}(V(s'))$ (I wrote $s_{t+1}$ as $s'$ to be consistent with the 2nd pdf and because writing $s_{t+1}$ is fucking stupid)\n",
    "3) Take the return $r_t$ in account and update $V_{new}(s) = V(s) + \\alpha* ( r(s) + \\gamma*\\text{arg max}_{s'}V(s') - V(s) )$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}